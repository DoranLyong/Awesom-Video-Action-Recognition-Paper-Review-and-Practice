# Dataset Lists & Review 

The following paper lists are what I did or will read, and they might not cover all papers published so far. If you want more paper lists, please refer to the links in the [Statistics](#Statistics) session. If you want more resources, you can refer to the [Papers-With-Code](https://paperswithcode.com/datasets?q=action&v=lst&o=match) or the [VisualData](https://visualdata.io/discovery) website. 

The datasets will be categorized depending on each target task: 

* Action Recognition (**AR**)
* Action Prediction (**AP**)
* Temporal Action Proposal (**TAP**)
* Temporal Action Localization/Detection (**TAL/D**)
* Spatiotemporal Action Proposal (**SAP**)
* Spatiotemporal Action Localization/Detection (**SAL/D**)



Or depending on what kind of **actors** and **annotations** :

- actors : 
  - Human (**H**)
  - Non-human (**N**)
- annotations : 
  - Action class (**C**)
  - Temporal markers (**T**)
  - Saptiotemporal bounding boxes/masks (**S**)
  - Audio Caption (**A**)



(:exclamation:) Papers which I will read soon are marked with  :last_quarter_moon:. 

---

## Table of Contents (ongoing)

* [Statistics](#Statistics) 
* [Conferences & Journals](#Conferences & Journals) 
  * 2021 update 
  * 2019 update
* Others 

---

## Statistics 

| Video Datset | Year | Action Classes | Action Instances | Actors | Annotations | Theme/Purpose |
|:-:           |:-:   |:-:|:-:|:-:|:-:|:-:           |
| Spoken Moments (S-MiT) | 2021 | 313 | ~500,000 | **H**, **N** | **C**, **A** | joint audio-visual |
| HAA500 | 2020.9 | 500 | ~10,000 | **H** | **C**, **T** | course-grained atomic actions |
| Multi-Moments in Time (Multi-MiT) - v2 | 2019 | 313 | ~1,020,000 | **H**, **N** | **C** | multi-label, extends MiT |
| Moments in Time (MiT) - v2 | 2019 | 339 | 1,000,000 | **H**, **N** | **C** | intra-class variation, web videos |



---

## Conferences & Journals 

>  ICCV, CVPR, 

> PAMI, 



### ICCV2021

> 1. HAA500: Human-Centric ```Atomic Action``` Dataset with Curated Videos, ICCV2021, [**AR**, **TAP**] [[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Chung_HAA500_Human-Centric_Atomic_Action_Dataset_With_Curated_Videos_ICCV_2021_paper.pdf)] [[homepage](https://www.cse.ust.hk/haa/)] [[3rd src](https://paperswithcode.com/paper/haa500-human-centric-atomic-action-dataset)] [[info](https://paperswithcode.com/dataset/haa500)] 



### CVPR2021

> 1. ```Spoken Moments```: Learning Joint Audio-Visual Representations From Video Descriptions, CVPR2021, [**AR**] [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Monfort_Spoken_Moments_Learning_Joint_Audio-Visual_Representations_From_Video_Descriptions_CVPR_2021_paper.html)] [[homepage](http://moments.csail.mit.edu/)] [[3rd src](https://paperswithcode.com/paper/spoken-moments-learning-joint-audio-visual)] [info]



### PAMI2021

> 1. ```Multi-Moments in Time```: Learning and Interpreting Models for Multi-Action Video Understanding, IEEE PAMI2021, [**AR**] [[paper](https://ieeexplore.ieee.org/abstract/document/9609554?casa_token=O24AhVQ-h_wAAAAA:62O8a9r2QMVDR3jbuWznk15jJFaojjDfXQEa1QuqzZyuaaEVSg_rqnjjh9tUe5zwWn3a1LtssVA)] [[homepage](http://moments.csail.mit.edu/)] [[free paper](https://arxiv.org/pdf/1911.00232.pdf)] [[3rd src](https://paperswithcode.com/paper/multi-moments-in-time-learning-and)] [info]



### PAMI2019

> 1. ```Moments in Time``` Dataset: One Million Videos for Event Understanding, IEEE PAMI2019, [**AR**] [[paper](https://ieeexplore.ieee.org/abstract/document/8651343?casa_token=WH9EuHBydqAAAAAA:MZvy3WTceUuqx6kSltyqpcako4J8w8Ih_vLg4DsUCSBJ4Wsy07ZWK3jt-GLwpDRAxLd2G1eFtXE)] [[homepage](http://moments.csail.mit.edu/)] [[free paper](http://moments.csail.mit.edu/TPAMI.2019.2901464.pdf)] [[3rd src](https://paperswithcode.com/paper/moments-in-time-dataset-one-million-videos)] [[info](https://paperswithcode.com/dataset/moments-in-time)] 

